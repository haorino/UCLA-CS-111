NAME: Siddharth Joshi
EMAIL: sjoshi804@gmail.com
ID: 105032378
SLIPDAYS: 0

QUESTIONS & ANSWERS:

QUESTION 2.3.1 - CPU time in the basic list implementation:
Where do you believe most of the CPU time is spent in the 1 and 2-thread list tests?
Why do you believe these to be the most expensive parts of the code?
Where do you believe most of the CPU time is being spent in the high-thread spin-lock tests?
Where do you believe most of the CPU time is being spent in the high-thread mutex tests?

Ans. In my opinion, the majority of CPU time in 1 and 2 threaded list tests
is spent in list operations (insert, length, lookup, delete (they all would
probably have similar CPU time usage in comparison to each other)). This is
because with 1 thread: there is 0 time spent trying to obtain a lock, and  
with 2 threads, contention is very low (time waiting for lock is almost negligible)
The most expensive part of the code, I believe, would be the section where keys
are looked up and then deleted as it is the only section involving 2 list operations
in each iteration. In high threaded tests spin-locked implementations, the spinning
i.e. the no-op ';' that occurs while the lock is obtained is where I believe the
most CPU time would be spent. Whereas for mutex-locked implementations, the high
number of threads would still cause the majority of CPU time to go into the list
operations as the thread wouldn't waste CPU time trying to obtain the lock, and
instead just be put to sleep.


QUESTION 2.3.2 - Execution Profiling:
Where (what lines of code) are consuming most of the CPU time when the spin-lock version of the list 
exerciser is run with a large number of threads?
Why does this operation become so expensive with large numbers of threads?

Ans. The timedLock() function that actually implements the acquiring of the spin
lock is where the majority of CPU time is spent with a large number of threads.
Specifically, it is lines 64-65 where the "while (__sync_lock_test_and_set(spinLock, 1) == 1) ; //Spin"
statements occur that take up most of the CPU time. These lines are where a thread
checks if a lock has been acquired and if so, spins i.e. keeps checking until the lock
is freed. With a large number of threads, there is higher contention, which results in
more threads having to wait and thus each thread having to wait for longer before
it can acquire the lock and carry out list operations. As more and more threads, contest
for the same locks, more CPU time is consumed simply spinning - trying to acquire these 
locks - thus making the operation extremely expensive with a large number of threads.

QUESTION 2.3.3 - Mutex Wait Time:
Look at the average time per operation (vs. # threads) and the average wait-for-mutex time (vs. #threads).
Why does the average lock-wait time rise so dramatically with the number of contending threads?
Why does the completion time per operation rise (less dramatically) with the number of contending threads?
How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?

Ans. The average lock time rises dramatically with the number of threads, as the contention
rises significant;y forcing threads to have to wait more often and for longer. Moreover,
The completion time per operations rises but not as quickly, since the context switches
between the threads would cause the data in the L1 and L2 Caches to get replaced. this
cases the time to rise but not as much as acceses to memory might or the spinning.
It is possible for the wait time per operation to go up faster (or higher) than the completion
time per operation in high-threaded runs. This is due to the high contention between threads
for the locks on the lists which would result in the overhead of threads waiting
for longer than it takes to complete the operation, thus causing the wait time per operation 
to be higher than the per operation completion time.

QUESTION 2.3.4 - Performance of Partitioned Lists
Explain the change in performance of the synchronized methods as a function of the number of lists.
Should the throughput continue increasing as the number of lists is further increased? If not, explain why not.
It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput 
of a single list with fewer (1/N) threads. Does this appear to be true in the above curves? If not, explain why not.

Ans. As more sub-lists are added, the threads can now insert the elements into any of the sub-lists
and since each sub list is distinct (not linked across sub lists in the linked list), they have 
a lock for each sub list. As a result, due to a good hash function that maps the elements to
one of the sublists, threads insert nearly the same amount of elements in each sub list.
This lowers the contention significantly, as all threads don't compete for a single resource
(the spin lock), rather they contend for one of the # of lists spin locks. This reduction 
in contention speeds up execution with more lists significantly. As the # of lists 
is further increased, there will be a dip in per operation time initially, but eventually
there will a point where adding more lists doesn't help as there is virtually no contention
amongst the different threads, thus the time taken per operation will not change. The throughput 
of an N-way partitioned list is equivalent to a single list with fewer (1/N) threads according 
to the graphs. This is due to the fact that the ratio of sub lists to threads is nearly the same 
amongst the 2 cases,and hence there is an equal amount of contention - leading to equivalent throughput.
For example, inserting 100 elements using 100 threads and 100 lists v/s inserting 100 elements
in 1 list using 1 thread would both have no contention assuming an ideal (a very good) hash function,
resulting in equal throughput. 


FILE DESCRIPTIONS:
lab2a_list.c: Driver for SortedList.c - does list operations with and without protection

SortedList.c: Implementation of function prototypes mentioned in SortedList.h

SortedList.h: Interface for Sorted List (provided w/o implementation in specifications)

Makefile: provides functionality to build, test, graphs, tests, profile and tar the files for distribution
build/default: compiles the files
tests: runs tests and logs them in 'csv's
graphs: creates graphs from the tests using GNU plot and the scripts given in project specifications 
dist: tars all files for distribution
default: compiles the files
tests: runs tests
profile: profiles the runtime of the operation and timedLock function in detail

lab2_list.gp: Scripts to plot the data

lab2_list.csv: Data obtained by running tests (make tests)

*.png: Graphs generated by make graphs

README: Explains features of the projects, answers questions and cites references. 

profile.out: output of make profile, stores the profiling of the spin lock test

REFERENCES:
https://en.wikipedia.org/wiki/Jenkins_hash_function
https://sourceware.org/binutils/docs/gprof/
https://people.duke.edu/~hpgavin/gnuplot.html

